{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematics\n",
    "### Boostrapping:\n",
    "Bootstrapping is a statistical resampling technique that involves random sampling of a dataset with replacement. To create bootstrapped dataset, we need to randomly choose rows from orginal data and duplication is allowed. In random forest, we only consider subset of features in bootstrapped dataset. So I use __bootstrap_size__ to indicate (__number of rows__, __number of features__)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code\n",
    "- Step 1: Create random bootstrapped datasets based on the number of trees.\n",
    "- Step 2: Put bootstrapped datasets into Decision Tree.\n",
    "- Step 3: Predict new dataset based on the average result of all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Question(object):\n",
    "    def __init__(self, column, condition):\n",
    "        self.column = column\n",
    "        self.condition = condition\n",
    "        \n",
    "    def match(self,row):\n",
    "        if isinstance(self.condition, str):\n",
    "            return row[self.column] == self.condition\n",
    "        else:\n",
    "            return row[self.column] >= self.condition\n",
    "         \n",
    "class LeafNode(object):\n",
    "    def __init__(self, label, samples, depth):\n",
    "        self.label = label\n",
    "        self.samples = samples\n",
    "        self.depth = depth\n",
    "\n",
    "class DecisionNode(object):\n",
    "    def __init__(self, question, true, false, samples, depth):\n",
    "        self.question = question\n",
    "        self.true = true\n",
    "        self.false = false\n",
    "        self.samples = samples\n",
    "        self.depth = depth\n",
    "        \n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth= 10, criterion = \"gini\", min_samples_split = 2, min_samples_leaf = 1):\n",
    "        self.train = pd.DataFrame()\n",
    "        self.test = pd.DataFrame()\n",
    "        self.label = []\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth  \n",
    "        self.min_samples_split = min_samples_split \n",
    "        self.min_samples_leaf = min_samples_leaf \n",
    "        self.tree = None\n",
    "        \n",
    "    def fit(self,train):\n",
    "        self.train = train\n",
    "        self.tree = self.build_tree(self.train, 0)\n",
    "        \n",
    "    # Step 1: Create a function to calculate the information gain of each question based on gini index or entropy\n",
    "    def impurity(self, label):\n",
    "        if self.criterion == \"gini\":\n",
    "            return 1 - ((label.value_counts()/label.value_counts().sum())**2).sum()\n",
    "        if self.criterion == \"entropy\":\n",
    "            p = label.value_counts()/label.value_counts().sum()\n",
    "            return - (p*np.log(p)).sum()      \n",
    "    \n",
    "    def info_gain(self, true_label, false_label, current_uncertainty):\n",
    "        p = float(len(true_label)) / (len(true_label) + len(false_label))\n",
    "        return current_uncertainty - p * self.impurity(true_label) - (1 - p) * self.impurity(false_label)\n",
    "    \n",
    "    # Step 2: Ask questions and split dataset into subnote\n",
    "    def split(self, data, question):  \n",
    "        if isinstance(question.condition, str):\n",
    "            true = data[data[question.column] == question.condition]\n",
    "            false = data[data[question.column] != question.condition]\n",
    "        else:\n",
    "            true = data[data[question.column] >= question.condition]\n",
    "            false = data[data[question.column] < question.condition]\n",
    "        return true,false\n",
    "    \n",
    "    # Step 3: Run a for loop through each value to find a best question - a question have highest information gain\n",
    "    def find_best_split(self, data):\n",
    "        best_gain = 0  \n",
    "        best_question = Question(None, None)\n",
    "        current_uncertainty = self.impurity(data[\"label\"])\n",
    "        for column in data.columns[:-1]:\n",
    "            for condition in data[column].unique():\n",
    "                true, false = self.split(data, Question(column, condition))\n",
    "                if len(true) == 0 or len(false) == 0:\n",
    "                    continue\n",
    "                gain = self.info_gain(true[\"label\"], false[\"label\"], current_uncertainty)\n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_question = gain, Question(column, condition)\n",
    "        return best_gain, best_question\n",
    "    \n",
    "    # Step 4: Use a recursive algorithm to build a tree\n",
    "    def build_tree(self, data, depth):\n",
    "        # Find best question         \n",
    "        gain, question = self.find_best_split(data)\n",
    "        samples = data[\"label\"].value_counts().sum()\n",
    "        # Can not find question or the samples is smaller than min samples split          \n",
    "        if gain == 0 or samples < self.min_samples_split or depth == self.max_depth:\n",
    "            label = (data[\"label\"].value_counts()/data[\"label\"].value_counts().sum()).apply(lambda x: int(x*100)).to_dict()\n",
    "            return LeafNode(label, samples, depth)\n",
    "        # Split based on best question         \n",
    "        true, false = self.split(data, question)\n",
    "        true_samples = true[\"label\"].value_counts().sum() \n",
    "        false_samples = false[\"label\"].value_counts().sum() \n",
    "        # Check if leaf node is smaller than min samples leaf or not\n",
    "        if true_samples < self.min_samples_leaf or false_samples < self.min_samples_leaf:\n",
    "            label = (data[\"label\"].value_counts()/data[\"label\"].value_counts().sum()).apply(lambda x: int(x*100)).to_dict()\n",
    "            return LeafNode(label, samples, depth)\n",
    "        true = self.build_tree(true, depth + 1)\n",
    "        false = self.build_tree(false, depth + 1)\n",
    "        return DecisionNode(question, true, false, samples, depth)\n",
    "    # Print tree\n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            print (spacing + \"Predict\", node.label, \", Samples: \", node.samples, \", Depth: \", node.depth)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.question.column) + \" \" + str(node.question.condition), \"Samples: \", node.samples, \", Depth: \", node.depth)\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        self.print_tree(node.true, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        self.print_tree(node.false, spacing + \"  \")\n",
    "    # Step 5: Predict new data based on the tree already built\n",
    "    def classify(self, index, node):\n",
    "        row = self.test.loc[index]\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            self.label.append(node.label)\n",
    "        # Decide whether to follow the true-branch or the false-branch.\n",
    "        # Compare the feature / value stored in the node,\n",
    "        # to the example we're considering.\n",
    "        if isinstance(node, DecisionNode):\n",
    "            if node.question.match(row):\n",
    "                return self.classify(index, node.true)\n",
    "            else:\n",
    "                return self.classify(index, node.false)\n",
    "    def predict(self, test):\n",
    "        self.test = test\n",
    "        for i in range(test.shape[0]):\n",
    "            self.classify(i, self.tree)\n",
    "        return self.label\n",
    "class RandomForest(object):\n",
    "    def __init__(self, boostrap_size, n_trees, max_depth= 10, criterion = \"gini\", min_samples_split = 2, min_samples_leaf = 1):\n",
    "            self.train = pd.DataFrame()\n",
    "            self.test = pd.DataFrame()\n",
    "            self.boostrap_size = boostrap_size\n",
    "            self.n_trees = n_trees\n",
    "            self.criterion = criterion\n",
    "            self.max_depth = max_depth  \n",
    "            self.min_samples_split = min_samples_split \n",
    "            self.min_samples_leaf = min_samples_leaf \n",
    "            self.forest = []\n",
    "    def fit(self,train):\n",
    "        self.train = train\n",
    "        self.create_forest()\n",
    "    def bootstrapping(self):\n",
    "        sample_indices = np.random.randint(low=0, high=self.train.shape[0], size = self.boostrap_size[0])\n",
    "        feature_indices = np.random.choice(self.train.shape[1]-1, self.boostrap_size[1], replace=False)\n",
    "        boostrap = self.train.iloc[sample_indices,np.append(feature_indices,-1)]\n",
    "        return boostrap\n",
    "    def create_forest(self):\n",
    "        for i in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth = self.max_depth, criterion = self.criterion, min_samples_split = self.min_samples_split, min_samples_leaf = self.min_samples_leaf)\n",
    "            tree.fit(self.bootstrapping())\n",
    "            self.forest.append(tree)\n",
    "    def predict(self, test):\n",
    "        self.test = test\n",
    "        labels = pd.DataFrame(np.zeros((self.test.shape[0], len(self.train[\"label\"].unique()))), columns = self.train[\"label\"].unique(), dtype = np.int32)\n",
    "        for tree in self.forest:\n",
    "            label = tree.predict(test)\n",
    "            for i in range(len(label)):\n",
    "                labels.loc[i][list(label[i].keys())[0]] += list(label[i].values())[0]\n",
    "        labels = labels/self.n_trees\n",
    "        labels = labels.astype(\"str\") + \"%\"\n",
    "        return pd.concat([self.test,labels], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Blue', 1, 'Berry'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Blue', 2, 'Berry'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 5, 'Banana'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Green', 4, 'Banana'],\n",
    "    ['Blue', 2, 'Berry'],\n",
    "]\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "data = pd.DataFrame(data = training_data, columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0\n",
      "diameter 2 Samples:  10 , Depth:  0\n",
      "--> True:\n",
      "  diameter 3 Samples:  7 , Depth:  1\n",
      "  --> True:\n",
      "    diameter 4 Samples:  5 , Depth:  2\n",
      "    --> True:\n",
      "      Predict {'Banana': 100} , Samples:  1 , Depth:  3\n",
      "    --> False:\n",
      "      Predict {'Lemon': 50, 'Apple': 50} , Samples:  4 , Depth:  3\n",
      "  --> False:\n",
      "    Predict {'Berry': 100} , Samples:  2 , Depth:  2\n",
      "--> False:\n",
      "  Predict {'Grape': 100} , Samples:  3 , Depth:  1\n",
      "Tree 1\n",
      "diameter 3 Samples:  10 , Depth:  0\n",
      "--> True:\n",
      "  diameter 4 Samples:  5 , Depth:  1\n",
      "  --> True:\n",
      "    Predict {'Banana': 100} , Samples:  2 , Depth:  2\n",
      "  --> False:\n",
      "    Predict {'Apple': 100} , Samples:  3 , Depth:  2\n",
      "--> False:\n",
      "  diameter 2 Samples:  5 , Depth:  1\n",
      "  --> True:\n",
      "    Predict {'Berry': 100} , Samples:  1 , Depth:  2\n",
      "  --> False:\n",
      "    Predict {'Grape': 75, 'Berry': 25} , Samples:  4 , Depth:  2\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = RandomForest((10,1), 2)\n",
    "model.fit(data)\n",
    "# Plot model\n",
    "for i in range(len(model.forest)):\n",
    "    print(\"Tree\", i)\n",
    "    model.forest[i].print_tree(model.forest[i].tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>diameter</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Grape</th>\n",
       "      <th>Lemon</th>\n",
       "      <th>Berry</th>\n",
       "      <th>Banana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Green</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>25.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>25.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>87.5%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>25.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color  diameter  Apple  Grape  Lemon Berry Banana\n",
       "0   Green         3  50.0%   0.0%  25.0%  0.0%   0.0%\n",
       "1  Yellow         3  50.0%   0.0%  25.0%  0.0%   0.0%\n",
       "2     Red         1   0.0%  87.5%   0.0%  0.0%   0.0%\n",
       "3     Red         3  50.0%   0.0%  25.0%  0.0%   0.0%"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "test = data.loc[0:3][[\"color\",\"diameter\"]]\n",
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. References\n",
    "QuantStart - Bootstrap Aggregation, Random Forests and Boosted Trees [https://www.quantstart.com/articles/bootstrap-aggregation-random-forests-and-boosted-trees]\n",
    "\n",
    "Youtube - StatQuest: Random Forests Part 1 - Building, Using and Evaluating[https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&t=313s]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
