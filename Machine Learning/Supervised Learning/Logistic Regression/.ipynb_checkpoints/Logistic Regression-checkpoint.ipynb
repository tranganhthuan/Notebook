{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematics\n",
    "### Logistic Regression:\n",
    "Logistic Regression in matrix form.\n",
    "$$f(\\mathbf{x}) = \\theta(\\mathbf{w}^T\\mathbf{x})$$\n",
    "Note: \n",
    "- $\\theta$ is logistic function (activation function).\n",
    "\n",
    "### Activation functions and its derivative:\n",
    "__Sigmoid function:__\n",
    "- Formula:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "- Derivative:\n",
    "$$\\sigma'(z) = \\left(\\frac{1}{1 + e^{-z}}\\right)'$$\n",
    "$$= \\frac{e^{-z}}{(1 + e^{-z})^2}$$ \n",
    "$$= \\frac{1}{(1 + e^{-z})} \\frac{e^{-z}}{(1 + e^{-z})}$$\n",
    "$$= \\frac{1}{(1 + e^{-z})} \\left(\\frac{1+e^{-z}}{(1 + e^{-z})}- \\frac{1}{1+e^{-z}}\\right)$$\n",
    "$$= \\sigma(z) (1-\\sigma(z))$$\n",
    "\n",
    "__Tanh function:__\n",
    "- Formula:\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "- Derivative:\n",
    "$$\\tanh'(z) = \\left(\\frac{e^z - e^{-z}}{e^z + e^{-z}}\\right)'$$\n",
    "$$= \\frac{(e^z - e^{-z})'(e^z + e^{-z}) - (e^z - e^{-z})(e^z + e^{-z})'}{(e^z + e^{-z})^2}$$ \n",
    "$$= \\frac{(e^z + e^{-z})(e^z + e^{-z}) - (e^z - e^{-z})(e^z - e^{-z})}{(e^z + e^{-z})^2}$$\n",
    "$$= \\left(\\frac{e^z + e^{-z}}{e^z + e^{-z}}\\right)^2 - \\left(\\frac{e^z - e^{-z}}{e^z + e^{-z}}\\right)^2$$\n",
    "$$= 1 - \\tanh(z)^2$$\n",
    "\n",
    "__ReLU function:__\n",
    "- Formula:\n",
    "$$\\text{ReLU}(z) = max(0,z)$$\n",
    "- Derivative:\n",
    "$$\\text{ReLU}(z)' = max(0,z)'$$\n",
    "$$= \\begin{cases} z' \\text{ if } z > 0 \\\\ 0' \\text{ if } otherwise \\end{cases}$$ \n",
    "$$= \\begin{cases} 1 \\text{ if } z > 0 \\\\ undefined &\\text{ if } z = 0 \\\\ 0 \\text{ if } z < 0 \\end{cases} $$\n",
    "\n",
    "###  Loss function:\n",
    "The loss function is entropy formula.\n",
    "$$J(\\mathbf{w}) = -\\log P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})$$\n",
    "$$= -\\sum_{i=1}^N(y_i \\log {z}_i + (1-y_i) \\log (1 - {z}_i))$$\n",
    "\n",
    "### Optimize:\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} &=& -(\\frac{y_i}{z_i} - \\frac{1- y_i}{1 - z_i} ) \\frac{\\partial z_i}{\\partial \\mathbf{w}} \\\\\n",
    "&=& \\frac{z_i - y_i}{z_i(1 - z_i)} \\frac{\\partial z_i}{\\partial \\mathbf{w}}\\\\\n",
    "&=& \\frac{z_i - y_i}{z_i(1 - z_i)} \\frac{\\partial z_i}{\\partial s} \\mathbf{x}\\\\\n",
    "\\end{eqnarray}$$\n",
    "Note:\n",
    "- $\\frac{\\partial z_i}{\\partial \\mathbf{w}} = \\frac{\\partial z_i}{\\partial s} \\frac{\\partial s}{\\partial \\mathbf{w}} = \\frac{\\partial z_i}{\\partial s} \\mathbf{x}$\n",
    "- $\\frac{\\partial z_i}{\\partial s} = f(a)'$  is the derivative calculated above with $a = \\mathbf{w}^T\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_predict = np.array([])\n",
    "        self.y_predict = np.array([])\n",
    "        self.theta = np.array([])\n",
    "        self.iteration = 0\n",
    "\n",
    "    def fit(self, X_train, y_train, iteration=2000):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.iteration = iteration\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1 + np.exp(-s))\n",
    "\n",
    "    def calculate_theta(self, eta=0.05):\n",
    "        X_bar = np.concatenate((np.ones((self.X_train.shape[0], 1)), self.X_train), axis=1)\n",
    "        N, d = X_bar.shape[0], X_bar.shape[1]\n",
    "        self.theta = np.zeros(d)\n",
    "        for iter in range(self.iteration):\n",
    "            self.theta -= eta*X_bar.T.dot((self.sigmoid(X_bar.dot(self.theta)) - self.y_train))\n",
    "\n",
    "    def predict(self, X_predict):\n",
    "        self.calculate_theta()\n",
    "        self.X_predict = X_predict\n",
    "        X_bar_predict = np.concatenate((np.ones((self.X_predict.shape[0], 1)), self.X_predict), axis=1)\n",
    "        self.y_predict = self.sigmoid(X_bar_predict.dot(self.theta))\n",
    "        return self.y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
