{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematics\n",
    "### Gini Index: \n",
    "$$I_G(P) = 1 - \\sum_{i=1}^J p_i^2$$\n",
    "### Entropy:\n",
    "$$H(\\mathbf{p}) = -\\sum_{i=1}^n p_i \\log(p_i)\\quad\\quad$$\n",
    "Note:\n",
    "- $\\mathbf{p} = \\frac{N_c}{N}$ with $N_c$ is the number of data in class $c$ and $N$ is the number of data in the whole dataset.\n",
    "### Information gain:\n",
    "$$IG(\\text{Question}) = I_G(\\text{Node}) - P(\\text{True}).I_G(\\text{True}) - P(\\text{False}).I_G(\\text{False})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code\n",
    "- Step 1: Create a function to calculate the information gain of each question based on gini index or entropy.\n",
    "- Step 2: Ask questions and split dataset into subnote.\n",
    "- Step 3: Run a for loop through each value to find a best question - a question have highest information gain.\n",
    "- Step 4: Use a recursive algorithm to build a tree.\n",
    "    - Base case: The node can not be asked more questions return Leaf Node.\n",
    "    - Recusive case: If there is still questions to ask, ask question and then check the child nodes - this node is considered as Decision Node. \n",
    "- Step 5: Predict new data based on the tree already built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Question(object):\n",
    "    def __init__(self, column, condition):\n",
    "        self.column = column\n",
    "        self.condition = condition\n",
    "        \n",
    "    def match(self,row):\n",
    "        if isinstance(self.condition, str):\n",
    "            return row[self.column] == self.condition\n",
    "        else:\n",
    "            return row[self.column] >= self.condition\n",
    "         \n",
    "class LeafNode(object):\n",
    "    def __init__(self, label, samples, depth):\n",
    "        self.label = label\n",
    "        self.samples = samples\n",
    "        self.depth = depth\n",
    "\n",
    "class DecisionNode(object):\n",
    "    def __init__(self, question, true, false, samples, depth):\n",
    "        self.question = question\n",
    "        self.true = true\n",
    "        self.false = false\n",
    "        self.samples = samples\n",
    "        self.depth = depth\n",
    "        \n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth= 10, criterion = \"gini\", min_samples_split = 2, min_samples_leaf = 1):\n",
    "        self.train = pd.DataFrame()\n",
    "        self.test = pd.DataFrame()\n",
    "        self.label = []\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth  \n",
    "        self.min_samples_split = min_samples_split \n",
    "        self.min_samples_leaf = min_samples_leaf \n",
    "        self.tree = None\n",
    "        \n",
    "    def fit(self,train):\n",
    "        self.train = train\n",
    "        self.tree = self.build_tree(self.train, 0)\n",
    "        self.print_tree(self.tree)\n",
    "        \n",
    "    # Step 1: Create a function to calculate the information gain of each question based on gini index or entropy\n",
    "    def impurity(self, label):\n",
    "        if self.criterion == \"gini\":\n",
    "            return 1 - ((label.value_counts()/label.value_counts().sum())**2).sum()\n",
    "        if self.criterion == \"entropy\":\n",
    "            p = label.value_counts()/label.value_counts().sum()\n",
    "            return - (p*np.log(p)).sum()      \n",
    "    \n",
    "    def info_gain(self, true_label, false_label, current_uncertainty):\n",
    "        p = float(len(true_label)) / (len(true_label) + len(false_label))\n",
    "        return current_uncertainty - p * self.impurity(true_label) - (1 - p) * self.impurity(false_label)\n",
    "    \n",
    "    # Step 2: Ask questions and split dataset into subnote\n",
    "    def split(self, data, question):  \n",
    "        if isinstance(question.condition, str):\n",
    "            true = data[data[question.column] == question.condition]\n",
    "            false = data[data[question.column] != question.condition]\n",
    "        else:\n",
    "            true = data[data[question.column] >= question.condition]\n",
    "            false = data[data[question.column] < question.condition]\n",
    "        return true,false\n",
    "    \n",
    "    # Step 3: Run a for loop through each value to find a best question - a question have highest information gain\n",
    "    def find_best_split(self, data):\n",
    "        best_gain = 0  \n",
    "        best_question = Question(None, None)\n",
    "        current_uncertainty = self.impurity(data[\"label\"])\n",
    "        for column in data.columns[:-1]:\n",
    "            for condition in data[column].unique():\n",
    "                true, false = self.split(data, Question(column, condition))\n",
    "                if len(true) == 0 or len(false) == 0:\n",
    "                    continue\n",
    "                gain = self.info_gain(true[\"label\"], false[\"label\"], current_uncertainty)\n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_question = gain, Question(column, condition)\n",
    "        return best_gain, best_question\n",
    "    \n",
    "    # Step 4: Use a recursive algorithm to build a tree\n",
    "    def build_tree(self, data, depth):\n",
    "        # Find best question         \n",
    "        gain, question = self.find_best_split(data)\n",
    "        samples = data[\"label\"].value_counts().sum()\n",
    "        # Can not find question or the samples is smaller than min samples split          \n",
    "        if gain == 0 or samples < self.min_samples_split or depth == self.max_depth:\n",
    "            label = (data[\"label\"].value_counts()/data[\"label\"].value_counts().sum()).apply(lambda x: str(int(x*100))+\"%\").to_dict()\n",
    "            return LeafNode(label, samples, depth)\n",
    "        # Split based on best question         \n",
    "        true, false = self.split(data, question)\n",
    "        true_samples = true[\"label\"].value_counts().sum() \n",
    "        false_samples = false[\"label\"].value_counts().sum() \n",
    "        # Check if leaf node is smaller than min samples leaf or not\n",
    "        if true_samples < self.min_samples_leaf or false_samples < self.min_samples_leaf:\n",
    "            label = (data[\"label\"].value_counts()/data[\"label\"].value_counts().sum()).apply(lambda x: str(int(x*100))+\"%\").to_dict()\n",
    "            return LeafNode(label, samples, depth)\n",
    "        true = self.build_tree(true, depth + 1)\n",
    "        false = self.build_tree(false, depth + 1)\n",
    "        return DecisionNode(question, true, false, samples, depth)\n",
    "    \n",
    "    # Print tree\n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            print (spacing + \"Predict\", node.label, \", Samples: \", node.samples, \", Depth: \", node.depth)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.question.column) + \" \" + str(node.question.condition), \"Samples: \", node.samples, \", Depth: \", node.depth)\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        self.print_tree(node.true, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        self.print_tree(node.false, spacing + \"  \")\n",
    "    \n",
    "    # Step 5: Predict new data based on the tree already built\n",
    "    def classify(self, index, node):\n",
    "        row = self.test.loc[index]\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            self.label.append(node.label)\n",
    "        # Decide whether to follow the true-branch or the false-branch.\n",
    "        # Compare the feature / value stored in the node,\n",
    "        # to the example we're considering.\n",
    "        if isinstance(node, DecisionNode):\n",
    "            if node.question.match(row):\n",
    "                return self.classify(index, node.true)\n",
    "            else:\n",
    "                return self.classify(index, node.false)\n",
    "    def predict(self, test):\n",
    "        self.test = test\n",
    "        for i in range(test.shape[0]):\n",
    "            self.classify(i, self.tree)\n",
    "        self.test[\"label\"] = self.label\n",
    "        return self.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Blue', 1, 'Berry'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Blue', 2, 'Berry'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 5, 'Banana'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Green', 4, 'Banana'],\n",
    "    ['Blue', 2, 'Berry'],\n",
    "]\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "data = pd.DataFrame(data = training_data, columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter 2 Samples:  16 , Depth:  0\n",
      "--> True:\n",
      "  diameter 4 Samples:  9 , Depth:  1\n",
      "  --> True:\n",
      "    Predict {'Banana': '100%'} , Samples:  2 , Depth:  2\n",
      "  --> False:\n",
      "    diameter 3 Samples:  7 , Depth:  2\n",
      "    --> True:\n",
      "      Predict {'Apple': '60%', 'Lemon': '40%'} , Samples:  5 , Depth:  3\n",
      "    --> False:\n",
      "      Predict {'Berry': '100%'} , Samples:  2 , Depth:  3\n",
      "--> False:\n",
      "  Predict {'Grape': '85%', 'Berry': '14%'} , Samples:  7 , Depth:  1\n"
     ]
    }
   ],
   "source": [
    "# Create and Plot model\n",
    "model = DecisionTree(min_samples_split = 3, min_samples_leaf = 2, max_depth = 3)\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>diameter</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Green</td>\n",
       "      <td>3</td>\n",
       "      <td>{'Apple': '60%', 'Lemon': '40%'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>3</td>\n",
       "      <td>{'Apple': '60%', 'Lemon': '40%'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Grape': '85%', 'Berry': '14%'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red</td>\n",
       "      <td>3</td>\n",
       "      <td>{'Apple': '60%', 'Lemon': '40%'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color  diameter                             label\n",
       "0   Green         3  {'Apple': '60%', 'Lemon': '40%'}\n",
       "1  Yellow         3  {'Apple': '60%', 'Lemon': '40%'}\n",
       "2     Red         1  {'Grape': '85%', 'Berry': '14%'}\n",
       "3     Red         3  {'Apple': '60%', 'Lemon': '40%'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "test = data.loc[0:3][[\"color\",\"diameter\"]]\n",
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube - Let’s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8: [https://www.youtube.com/watch?v=LDRbO9a6XPU]\n",
    "\n",
    "Machine Learning cơ bản - Bài 34: Decision Trees (1): Iterative Dichotomiser 3 [https://machinelearningcoban.com/2018/01/14/id3/]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
