{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematics\n",
    "### Gini impurity: \n",
    "$$I_G(P) = 1 - \\sum_{i=1}^J p_i^2$$\n",
    "\n",
    "### Information gain:\n",
    "$$IG(\\text{Question}) = I_G(\\text{Node}) - P(\\text{True}).I_G(\\text{True}) - P(\\text{False}).I_G(\\text{False})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code\n",
    "- Step 1: Create a function to calculate the information gain of each question based on gini index or entropy.\n",
    "- Step 2: Ask questions and split dataset into subnote.\n",
    "- Step 3: Run a for loop through each value to find a best question - a question have highest information gain.\n",
    "- Step 4: Use a recursive algorithm to build a tree.\n",
    "    - Base case: The node can not be asked more questions return Leaf Node.\n",
    "    - Recusive case: If there is still questions to ask, ask question and then check the child nodes - this node is considered as Decision Node. \n",
    "- Step 5: Predict new data based on the tree already built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Question(object):\n",
    "    def __init__(self, column, condition):\n",
    "        self.column = column\n",
    "        self.condition = condition\n",
    "        \n",
    "    def match(self,row):\n",
    "        if isinstance(self.condition, str):\n",
    "            return row[self.column] == self.condition\n",
    "        else:\n",
    "            return row[self.column] >= self.condition\n",
    "         \n",
    "class LeafNode(object):\n",
    "    def __init__(self, label, samples):\n",
    "        self.label = label\n",
    "        self.samples = samples\n",
    "\n",
    "class DecisionNode(object):\n",
    "    def __init__(self, question, true, false, samples):\n",
    "        self.question = question\n",
    "        self.true = true\n",
    "        self.false = false\n",
    "        self.samples = samples\n",
    "        \n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth= 10, criterion = \"gini\", min_samples_split = 2, min_samples_leaf = 1):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth  \n",
    "        self.min_samples_split = min_samples_split \n",
    "        self.min_samples_leaf = min_samples_leaf \n",
    "        self.tree = None\n",
    "        \n",
    "    def fit(self,data):\n",
    "        self.data = data\n",
    "        self.tree = self.build_tree(self.data)\n",
    "        self.print_tree(self.tree)\n",
    "        \n",
    "    # Step 1: Create a function to calculate the information gain of each question based on gini index or entropy\n",
    "    def impurity(self, label):\n",
    "        if self.criterion == \"gini\":\n",
    "            return 1 - ((label.value_counts()/label.value_counts().sum())**2).sum()\n",
    "        if self.criterion == \"entropy\":\n",
    "            p = label.value_counts()/label.value_counts().sum()\n",
    "            return - (p*np.log(p)).sum()      \n",
    "    \n",
    "    def info_gain(self, true_label, false_label, current_uncertainty):\n",
    "        p = float(len(true_label)) / (len(true_label) + len(false_label))\n",
    "        return current_uncertainty - p * self.impurity(true_label) - (1 - p) * self.impurity(false_label)\n",
    "    \n",
    "    # Step 2: Ask questions and split dataset into subnote\n",
    "    def split(self, data, question):  \n",
    "        if isinstance(question.condition, str):\n",
    "            true = data[data[question.column] == question.condition]\n",
    "            false = data[data[question.column] != question.condition]\n",
    "        else:\n",
    "            true = data[data[question.column] >= question.condition]\n",
    "            false = data[data[question.column] < question.condition]\n",
    "        return true,false\n",
    "    \n",
    "    # Step 3: Run a for loop through each value to find a best question - a question have highest information gain\n",
    "    def find_best_split(self, data):\n",
    "        best_gain = 0  \n",
    "        best_question = Question(None, None)\n",
    "        current_uncertainty = self.impurity(data[\"label\"])\n",
    "        for column in data.columns[:-1]:\n",
    "            for condition in data[column].unique():\n",
    "                true, false = self.split(data, Question(column, condition))\n",
    "                if len(true) == 0 or len(false) == 0:\n",
    "                    continue\n",
    "                gain = self.info_gain(true[\"label\"], false[\"label\"], current_uncertainty)\n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_question = gain, Question(column, condition)\n",
    "        return best_gain, best_question\n",
    "    \n",
    "    # Step 4: Use a recursive algorithm to build a tree\n",
    "    def build_tree(self, data):\n",
    "        gain, question = self.find_best_split(data)\n",
    "        samples = data[\"label\"].value_counts().sum()\n",
    "        if gain == 0:\n",
    "            label = (data[\"label\"].value_counts()/data[\"label\"].value_counts().sum()).apply(lambda x: str(int(x*100))+\"%\").to_dict()\n",
    "            samples = data[\"label\"].value_counts().sum()\n",
    "            return LeafNode(label, samples)\n",
    "        true, false = self.split(data, question)\n",
    "        true = self.build_tree(true)\n",
    "        false = self.build_tree(false)\n",
    "        return DecisionNode(question, true, false, samples)\n",
    "    \n",
    "    # Print tree\n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            print (spacing + \"Predict\", node.label, \"Samples: \", node.samples)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print (spacing + str(node.question.column) + \" \" + str(node.question.condition), \"Samples: \", node.samples)\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print (spacing + '--> True:')\n",
    "        print_tree(node.true, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print (spacing + '--> False:')\n",
    "        print_tree(node.false, spacing + \"  \")\n",
    "    \n",
    "    # \n",
    "    def classify(self, row, node):\n",
    "        # Base case: we've reached a leaf\n",
    "        if isinstance(node, LeafNode):\n",
    "            return node.label\n",
    "\n",
    "        # Decide whether to follow the true-branch or the false-branch.\n",
    "        # Compare the feature / value stored in the node,\n",
    "        # to the example we're considering.\n",
    "        if node.question.match(row):\n",
    "            return self.classify(row, node.true)\n",
    "        else:\n",
    "            return self.classify(row, node.false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "data = pd.DataFrame(data = training_data, columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter 3 Samples:  5\n",
      "--> True:\n",
      "  color Yellow Samples:  3\n",
      "  --> True:\n",
      "    Predict {'Lemon': '50%', 'Apple': '50%'} Samples:  2\n",
      "  --> False:\n",
      "    Predict {'Apple': '100%'} Samples:  1\n",
      "--> False:\n",
      "  Predict {'Grape': '100%'} Samples:  2\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTree()\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': '100%'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify(data.loc[0], model.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube - Let’s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes #8: [https://www.youtube.com/watch?v=LDRbO9a6XPU]\n",
    "\n",
    "Machine Learning cơ bản - Bài 34: Decision Trees (1): Iterative Dichotomiser 3 [https://machinelearningcoban.com/2018/01/14/id3/]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
